\documentclass{article}
\usepackage{amsmath, amssymb}
\usepackage{algorithm}
\usepackage{algpseudocode}

% 定义自用算子
\newcommand{\rowmax}{\operatorname{rowmax}}
\newcommand{\rowsum}{\operatorname{rowsum}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\softmax}{\operatorname{softmax}}

\begin{document}

\section{Preliminaries}
\subsection{Neural Network Quantization}

\begin{algorithm}[H]
  \caption{Standard Attention Implementation}
  \begin{algorithmic}[1]
  \Require Matrices $Q,K,V\in\mathbb{R}^{N\times d}$ in HBM.
  \State Load $Q,K$ by blocks from HBM, compute $S=QK^\top$, write $S$ to HBM.
  \State Read $S$ from HBM, compute $P=\softmax(S)$, write $P$ to HBM.
  \State Load $P$ and $V$ by blocks from HBM, compute $O=PV$, write $O$ to HBM.
  \State \Return $O$.
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
  \caption{FlashAttention}
  \begin{algorithmic}[1]
  \Require Matrices $Q,K,V\in\mathbb{R}^{N\times d}$ in HBM, on-chip SRAM of size $M$.
  \State Set block sizes $B_c=\Big\lfloor\frac{M}{4d}\Big\rfloor$, $B_r=\min\!\Big(\Big\lfloor\frac{M}{4d}\Big\rfloor,d\Big)$.
  \State Initialize $O=\mathbf{0}_{N\times d}\in\mathbb{R}^{N\times d}$ in HBM, $\ell=(0)_N\in\mathbb{R}^N$, $m=(-\infty)_N\in\mathbb{R}^N$ in HBM.
  \State Divide $Q$ into $T_r=\Big\lceil\frac{N}{B_r}\Big\rceil$ blocks $Q_1,\dots,Q_{T_r}$ of size $B_r\times d$ each, and divide $K,V$ into $T_c=\Big\lceil\frac{N}{B_c}\Big\rceil$ blocks $K_1,\dots,K_{T_c}$ and $V_1,\dots,V_{T_c}$ of size $B_c\times d$ each.
  \State Divide $O$ into $T_r$ blocks $O_1,\dots,O_{T_r}$ of size $B_r\times d$ each, divide $\ell$ into $T_r$ blocks $\ell_1,\dots,\ell_{T_r}$ of size $B_r$ each, divide $m$ into $T_r$ blocks $m_1,\dots,m_{T_r}$ of size $B_r$ each.
  \For{$1 \le j \le T_c$}
    \State Load $K_j, V_j$ from HBM to on-chip SRAM.
    \For{$1 \le i \le T_r$}
      \State Load $Q_i, O_i, \ell_i, m_i$ from HBM to on-chip SRAM.
      \State On chip, compute $S_{ij}=Q_i K_j^\top \in \mathbb{R}^{B_r\times B_c}$.
      \State On chip, compute $\tilde m_{ij}=\rowmax(S_{ij})\in\mathbb{R}^{B_r}$, $\;\tilde P_{ij}=\exp(S_{ij}-\tilde m_{ij})\in\mathbb{R}^{B_r\times B_c}$ (pointwise), $\;\tilde\ell_{ij}=\rowsum(\tilde P_{ij})\in\mathbb{R}^{B_r}$.
      \State On chip, compute $m_i^{\text{new}}=\max(m_i,\tilde m_{ij})\in\mathbb{R}^{B_r}$, $\;\ell_i^{\text{new}}=e^{m_i-m_i^{\text{new}}}\ell_i + e^{\tilde m_{ij}-m_i^{\text{new}}}\tilde\ell_{ij}\in\mathbb{R}^{B_r}$.
      \State Write $O_i \leftarrow \diag(\ell_i^{\text{new}})^{-1}\!\left(\diag(\ell_i)\,e^{m_i-m_i^{\text{new}}} O_i + e^{\tilde m_{ij}-m_i^{\text{new}}}\tilde P_{ij} V_j\right)$ to HBM.
      \State Write $\ell_i \leftarrow \ell_i^{\text{new}}$, $\; m_i \leftarrow m_i^{\text{new}}$ to HBM.
    \EndFor
  \EndFor
  \State \Return $O$.
  \end{algorithmic}
\end{algorithm}

\end{document}
