SOSP '25, October 13â€“16, 2025, Seoul, Republic of Korea

Yunchi Lu, Youshan Miao, Cheng Tan, Peng Huang, Yi Zhu, Xian Zhang, and Fan Yang

Group Controlled Variable

global batch size
hidden dimension size
sequence length
heads

gbs hidden seqlen heads pp layers dp mb tp
2
*
2
512
2
32
2
512
2
pipeline parallelism 512
8
512
8
512
8
512
*
512

layers
data parallelism
micro batches
tensor parallelism

4096
*
128
4096
4096
4096
4096
4096
4096

128
128
*
128
128
128
128
128
128

32
32
32
*
32
32
32
32
32

32
32
32
32
32
*
32
32
32

2
2
2
2
2
16
*
16
16

2
2
2
2
2
8
8
*
8

2
2
2
2
*
4
4
4
4

b
c
d
e
f
g
h
i
j

Table 7. Configurations for variable controlled experiments.

CP 1 to partially counteract the anti-scaling 2 introduced
by weight reducers in data parallelism 3 , ensuring that the
final updates reflect per-input-sequence gradients. However,
when calculate-per-token-loss is enabled, the averaging
across the DPÃ—CP communication group is skipped and instead
replaced by averaging over the total number of trained tokens
4 . In this case, the combination of 1 and 4 results in the
final gradients being over-scaled by a factor of CP.

TrainVerify can eliminate such bugs by comparing data
flow of shape-reduced symbolic tensors. While the violation
could be detected earlier via ğ¿ == ğ¿0, practical implementa-
tions typically do not enforce strict equivalence on distributed
losses. Moreover, adapted graphs from manually-parallelized
models lack the backward lineage, e.g. ğ‘¡ L
== (ğ‘¡0, ğ‘¡1) that
is naturally preserved by auto-parallel systems. As a result,
TrainVerify detects the problem as soon as it visits a weight
tensor in backward pass, (e.g., ğ‘”ğ‘¤0) by checking that its final-
ized gradient ğº0 is consistent with ğº.

Such computational issues are subtle, making diagnosis
particularly challenging, especially when the code spans mul-
tiple modules. The issue post reflects a 10-day effort involving
users, developers, and volunteers to reproduce the problem
and identify its root cause, amid early misdiagnoses and user
concerns. In the year prior to that fix, over 5 issues were filed
on the same code snippets, across various training configura-
tions; some were misreported, while others were resolved after
extensive discussion. TrainVerify can effectively alleviate
such challenges and ensure verified correctness.

C Shape reduction correctness proof
For complete correctness proof of shape reduction, please refer
to our external document at https://arxiv.org/abs/2506.15961.
CT: minor: - use bold for tensors; normal for scalar -
0-indexing vs. 1-based indexing (using (cid:174)0 seems indicating
0-based) - indexing is in I not R.

When a DNN model involves large size tensors, such as
popular LLMs, it becomes infeasible for current solvers (e.g.,
Z3) to verify its parallel execution considering the complexity,
as tensors now have symbolic elements. In response, we
propose a verification for the same model architecture but

YM: consider
Natural Num-
bers N or
Integers Z
rather than I

with reduced tensor shapes, and prove that: the verification
conclusion on the shape-reduced model also applies to the
original model with larger tensor shapes.

C.1 Formalization
A DNN model consists of multiple operators, such as MatMul
and ReLU, which essentially are functions with data tensors
as input and output. Given input tensor(s), including tensors
representing weights, activation and optimizer state, such a
DNN function can produce corresponding output tensor(s).
Given a DNN function ğ‘“ that executes on a single device,
there is an alternative function ğ‘” (ğ‘” is different from ğ‘“ ) that
can execute on either single device sequentially or multiple
devices concurrently. Our goal is to verify that regardless of
the inputs, ğ‘“ and ğ‘” can always produce the same resultsâ€”an
equivalence.

Definition 8 (Tensor). A tensor is an object that generalizes
scalars, vectors, and matrices to higher dimensions. Formally,
an order-ğ‘› tensor (also called an ğ‘›-th order tensor) is an
element of the tensor space:

T âˆˆ Rğ‘‘1 Ã—ğ‘‘2 Ã—Â·Â·Â·Ã—ğ‘‘ğ‘›
(1)
where R represents real number field and ğ‘‘1, ğ‘‘2, . . . , ğ‘‘ğ‘›

denote the dimensions along each mode of the tensor.

Tensors are used as primitive data in machine learning.

ğ‘“ : (Rğ‘‘ğ‘

Definition 9 (Functions). A general function that operates on
multiple tensors can be defined as:
2 Ã—Â·Â·Â·Ã—ğ‘‘ğ‘
2 Ã—Â·Â·Â·Ã—ğ‘‘ğ‘

ğ‘˜
(2)
where ğ‘“ takes one or more tensors as input and outputs a
tensor of a potentially different shape.

ğ‘š, Â· Â· Â· ) â†’ Rğ‘‘ ğ‘¦

2 Ã—Â·Â·Â·Ã—ğ‘‘ ğ‘¦

ğ‘› , Rğ‘‘ğ‘

1 Ã—ğ‘‘ ğ‘¦

1 Ã—ğ‘‘ğ‘

1 Ã—ğ‘‘ğ‘

For simplicity, we assume a single-tensor input/output for
function ğ‘“ throughout this proof. The proof can be naturally
extended to accommodate multiple input and output tensors.
We use bold symbols (e.g., x, y) to denote tensors and
non-bold symbols (e.g., ğ‘¥, ğ‘¦) to denote scalars. We also use
zero-based indexing; that is, for a vector v, the first element is
"v[0]".

TrainVerify: Equivalence-Based Verification for Distributed LLM Training

SOSP '25, October 13â€“16, 2025, Seoul, Republic of Korea

People have long observed that deep learning operators like
element-wise operations and convolution are SIMD (Single-
Instruction Multiple-Data): the operation consists of repeated,
homogeneous computations (the "kernel") over array elements.
This SIMD characteristic is the core enabler for our shape
reduction mechanism. Below, we formally define what is a
SIMD function.

Consider a function ğ‘“ (x) â†’ y, where x âˆˆ ğ‘…ğ‘‘ğ‘

1 Ã—ğ‘‘ğ‘

2 Ã—Â·Â·Â·Ã—ğ‘‘ğ‘

ğ‘š

and y âˆˆ ğ‘…ğ‘‘ğ‘

1 Ã—ğ‘‘ğ‘

2 Ã—Â·Â·Â·Ã—ğ‘‘ğ‘

ğ‘› . So, ğ‘Ÿğ‘ğ‘›ğ‘˜ (x) = ğ‘š and ğ‘Ÿğ‘ğ‘›ğ‘˜ (y) = ğ‘›.

If ğ‘“ is a SIMD function, a kernel function ğœƒ associated
with ğ‘“ takes a subtensor from x and outputs a scalar value.
Formally:

Definition 10 (Kernel function). A kerenel function ğœƒ is a
function that takes ğ‘˜ scalar inputs and produces a single
scalar output:

ğœƒ : Rğ‘˜ â†’ R.

Next, we define which input subtensor is associated with
each output element. Consider the same function ğ‘“ (x) â†’ y.
A dependency mapping ğœ associated with ğ‘“ is a function that
maps each index i in the output y to a list of indices in the
input x. Formally:

Definition 11 (Dependency mapping). A dependency map-
ping ğœ is an affine transformation that maps a vector of integers
(an index of tensor y) to a list of indices in another tensor (i.e.,
x):

ğœ : ğ‘–ğ‘‘ğ‘¥ (y) âˆˆ Nğ‘› â†’ [ğ‘–ğ‘‘ğ‘¥ (x), . . . ] âˆˆ Nğ‘˜ Ã—ğ‘š,

where ğ‘–ğ‘‘ğ‘¥ (Â·) is the indexing function of the tensor; ğ‘› and ğ‘š
are ranks of x and y; and ğ‘˜ is the number of inputs in ğœƒ .

With dependency mapping and kernel function, we define

SIMD functions.

Definition 12 (SIMD function). A function ğ‘“ (x) â†’ y is a
SIMD function if, for each y[i], i âˆˆ Nğ‘›,

y[i] = ğœƒ (x1, x2, . . . , xğ‘˜ ),

where ğœƒ is the kernel function of ğ‘“ , and

xğ‘— = x[ğœ (i) [ ğ‘—]],

1 â‰¤ ğ‘— â‰¤ ğ‘˜m

where ğœ is the dependency mapping of ğ‘“ .

By fixing the latent representation â€“ kernel function ğœƒ
and a dependency mapping ğœ â€“ one can define an SIMD
function. We denote an SIMD function ğ‘“ using its ğœƒ ğ‘“ and ğœğ‘“
as: y[i] = ğœƒ ğ‘“ (x[ğœğ‘“ (i)]).

Finally, we introduce another class of operators, reductional
opreations, such as sum. A reductional function ğ‘“ : Rğ‘š â†’ R
returns a single output element from processing a reductional
operation among all elements in the input tensor, with the
operation satisfying the commutative and associative laws.

Definition 13 (Reductional function). For an input tensor
x âˆˆ Rğ‘š, the reductional function ğ‘“âŠ™ applies a binary operation
âŠ™ to all elements of x such that:

ğ‘“âŠ™ (x) = x[0] âŠ™ x[1] âŠ™ Â· Â· Â· âŠ™ x[ğ‘š âˆ’ 1],

and âŠ™ satisfies commutativity (ğ‘ âŠ™ ğ‘ = ğ‘ âŠ™ ğ‘) and associativity
((ğ‘ âŠ™ ğ‘) âŠ™ ğ‘ = ğ‘ âŠ™ (ğ‘ âŠ™ ğ‘)).

C.2 Observations: LLM operators are SIMD functions
Deep Neural Network (DNN) computations are characterized
by their application to high-dimensional data tensors. A closer
examination of commonly used DNN operations reveals that
a large number of elements in the output tensor share the
same computational logic, differing only in the specific input
elements they process. This computational pattern aligns
closely with our definition of SIMD functions.

C.2.1 Observation 1: LLM operators have kernel func-
tions. We observe that each computation operator in the
transformer architecture is associated with its own kernel
function, including Feed Forward layers, Multi-Head Atten-
tion layers (without masking), Add & Norm layers, ReLU,
Softmax, and Residual Addition.

Consider matrix multiplication (i.e., MatMul) as an example.
Given two matrices A âˆˆ Rğ‘šÃ—ğ‘ and B âˆˆ Rğ‘ Ã—ğ‘›, the resulting
matrix C âˆˆ Rğ‘šÃ—ğ‘› has elements ğ‘ğ‘–,ğ‘— (short for C[ğ‘–] [ ğ‘—]) com-
puted by: ğ‘ğ‘–,ğ‘— = (cid:205)ğ‘
ğ‘ğ‘–,ğ‘˜ Â·ğ‘ğ‘˜,ğ‘— . Therefore, MatMul has a kernel
function:

ğ‘˜=1

ğœƒ (ğ‘ğ‘–,1, . . . , ğ‘ğ‘–,ğ‘, ğ‘1,ğ‘—, . . . , ğ‘ğ‘,ğ‘— ) = (cid:205)ğ‘

ğ‘˜=1

ğ‘ğ‘–,ğ‘˜ Â· ğ‘ğ‘˜,ğ‘—

C.2.2 Observation 2: dependency mappings in LLM op-
erators share linear components. This property is intuitive,
as the "striding" of kernel functions across tensors typically
occurs at regular, constant intervals. Consequently, when
the input to the dependency mappingâ€”corresponding to the
output tensor's indexâ€”changes, the resulting input indices
change linearly and follow the same pattern. That is, for each
input tensor, the mapping takes the affline transformations:

ğœ (i) = [M Â· i + b1,

. . . , M Â· i + bğ‘˜ ].

For example, in the above MatMul case, the dependency
mapping ğœğ´ for the first input matrix A can be written as affine
transformations:

YC: we need
to revisit
the example
as Mat-
mul=simd+redux+simd

ğœğ´ (

(cid:21)

(cid:20)ğ‘–
ğ‘—

) = [Mğ´

(cid:21)

(cid:20)ğ‘–
ğ‘—

Mğ´ =

(cid:18)1
0

(cid:19)

0
0

, bğ´1 =

+ bğ´1, . . . , Mğ´
(cid:18)0
1

, . . . , bğ´ğ‘ =

(cid:19)

(cid:19)

(cid:18)0
ğ‘

(cid:21)

(cid:20)ğ‘–
ğ‘—

+ bğ´ğ‘ ], where

Above all, MatMul is a SIMD function because it has

â€¢ a kernel function:

ğœƒ (ğ‘ğ‘–,1, . . . , ğ‘ğ‘–,ğ‘, ğ‘1,ğ‘—, . . . , ğ‘ğ‘,ğ‘— ) = (cid:205)ğ‘

ğ‘˜=1

ğ‘ğ‘–,ğ‘˜ Â· ğ‘ğ‘˜,ğ‘— ;

SOSP '25, October 13â€“16, 2025, Seoul, Republic of Korea

Yunchi Lu, Youshan Miao, Cheng Tan, Peng Huang, Yi Zhu, Xian Zhang, and Fan Yang

â€¢ a dependency mapping for each input tensor:

ğœğ´ ([ğ‘–, ğ‘—]) = [[ğ‘–, ğ‘˜]|1 â‰¤ ğ‘˜ â‰¤ ğ‘],
ğœ ğµ ([ğ‘–, ğ‘—]) = [[ğ‘˜, ğ‘—]|1 â‰¤ ğ‘˜ â‰¤ ğ‘],
where ğœğ´ and ğœ ğµ are dependency mappings for input matrix
A and B;

â€¢ and MatMul can be expressed as:

ğ‘ğ‘–,ğ‘— = ğœƒ (A[ğœğ´ ([ğ‘–, ğ‘—])] âŠ• B[ğœ ğµ ([ğ‘–, ğ‘—])]),

where âŠ• represents vector concatenation.

Fact 5. We observe that in practice, the dependency mapping
ğœ (Â·) does not produce duplicated input indices. Meaning,

âˆ€i, ğœ (i) = [j1, j2, . . . , jğ‘˜ ] âˆˆ Nğ‘˜ Ã—ğ‘š,
for 1 â‰¤ ğ‘ â‰  ğ‘ â‰¤ ğ‘˜ in ğœ (i), jğ‘ â‰  jğ‘ .

Essentially, all elements in the inputs to the well-formed
kernel functions contribute to the final output. There is no
such input element that does not influcent the output.

C.3.2 Premises from SMT solver. In TrainVerify, we
use an SMT solver (Z3) to verify that a shape-reduced model
preserves parallelization equivalence. Specifically, if the solver
returns sat, it proves that for all inputs, the logical dataflow
graph of the shape-reduced model is equivalent to that of the
parallelized version.

This result yields a premise for each stage (Â§5.2) in Train-

Verify of the form:

âˆ€x, âˆ€i âˆˆ I, ğ‘“ (x) [i] = ğ‘”(x) [i],

where I = {

ğ‘›
âˆ‘ï¸

ğ‘—=0

ğ‘ ğ‘— eğ‘—

| ğ‘ ğ‘— âˆˆ {0, 1} for all ğ‘— }

YC: we
may need a
clearer sep-
aration/clar-
ification of
whether our
SIMD include
the reduc-
tional op
(norm)

C.3 Correctness proof for shape reduction
This section establishes the correctness of TrainVerify's
shape reduction by proving the equivalence between two data
flow graphs (DFGs) at a reduced scale implies equivalence
at the original scale. We denote the original and transformed
DFGsâ€”before and after applying parallelization techniquesâ€”
as functions ğ‘“ and ğ‘”, respectively.

C.3.1 Prerequisite relations. Before presenting the main
theorem, we begin with two equivalent definitions that serve
as the foundation for the proof.

Definition 14 (Mapping permutation equivalence). For two
dependency mappings ğœ1 and ğœ2, we call them mapping per-
mutation equivalence, denoted ğœ1 (cid:27)ğ‘ƒ ğœ2, if there exists a
permutation function ğ‘ƒ, such that

âˆ€ğ‘–, ğ‘ƒ (ğœ1(ğ‘–)) = ğœ2(ğ‘–)

Mapping permutation equivalence captures LLM operators
with commutative and associative properties, where permuting
the inputs does not affect the output. Similarly, we need
to define a corresponding equivalence relation for kernel
functions.

Definition 15 (Kernel permutation-set equivalence). For two
kernel functions ğœƒ1 and ğœƒ2, we call them kernel permutation-
set equivalence, denoted ğœƒ1 (cid:27)ğ‘„ ğœƒ2, if there exists a non-empty
set ğ‘„ of permutation functions, such that

âˆ€ğ‘ƒ âˆˆ ğ‘„, âˆ€x, ğœƒ1(x) = ğœƒ2(ğ‘ƒ (x))

Definition 16 (Well-formed kernel function). We call a kernel
function ğœƒ well-formed if,

âˆƒx, xâ€², âˆ€ğ‘–, x[ğ‘–] â‰  xâ€² [ğ‘–] and âˆ€ğ‘— â‰  ğ‘–, x[ ğ‘—] = xâ€² [ ğ‘—]

ğœƒ (x) â‰  ğœƒ (xâ€²)

YC: the
"from" and
"to" relation
of the equa-
tion is not
clear; the
expression
is not well
aligned with
the def se-
mantics

In the equation, eğ‘– denotes the standard basis vectors in Rğ‘›,
defined as:

(eğ‘– ) ğ‘— =

(cid:40)

if ğ‘— = ğ‘–,
1
0 otherwise.

Each eğ‘– âˆˆ Nğ‘› is a column vector with a single 1 in the i-th
position and 0 elsewhere, except for ğ‘’0 which is all 0s. For
example,

0
...
e0 = (cid:169)
(cid:173)
(cid:173)
0
(cid:171)

1
...
, e1 = (cid:169)
(cid:173)
(cid:173)
0
(cid:171)

0
...
, . . . , eğ‘› = (cid:169)
(cid:173)
(cid:173)
1
(cid:171)

(cid:170)
(cid:174)
(cid:174)
(cid:172)ğ‘›Ã—1

(cid:170)
(cid:174)
(cid:174)
(cid:172)ğ‘›Ã—1

(cid:170)
(cid:174)
(cid:174)
(cid:172)ğ‘›Ã—1
The above premise holds due to Algorithm 2, line 12,
where TrainVerify enforces that, for any output dimension
of each operatorâ€”excluding those not involved in computation
(e.g., batch dimensions, or all dimensions in element-wise
operations)â€”both the logical and parallelized dataflow graphs
retain a size of at least two in those dimensions. Meanwhile,
the equivalence for abitrary input x is established by the
symbolic computation.

C.3.3 Main proofs. We now present the main proof of
shape reduction correctness. The argument proceeds in three
steps:
1. We first prove ğœƒ ğ‘“ (cid:27)ğ‘„ ğœƒğ‘” given the above premise.
2. We then prove ğœğ‘“ (cid:27)ğ‘ƒ ğœğ‘” based on the premise.
3. Finally, we apply ğœƒ ğ‘“ (cid:27)ğ‘„ ğœƒğ‘” and ğœğ‘“ (cid:27)ğ‘ƒ ğœğ‘” to establish the

shape reduction theorem.

Next, we consider ğ‘“ (x) â†’ y and ğ‘”(x) â†’ yâ€², where x âˆˆ
ğ‘…ğ‘‘ğ‘
2 Ã—Â·Â·Â·Ã—ğ‘‘ğ‘
ğ‘› . So, ğ‘Ÿğ‘ğ‘›ğ‘˜ (x) = ğ‘š
and ğ‘Ÿğ‘ğ‘›ğ‘˜ (y) = ğ‘›.

ğ‘š and y, yâ€² âˆˆ ğ‘…ğ‘‘ğ‘

2 Ã—Â·Â·Â·Ã—ğ‘‘ğ‘

1 Ã—ğ‘‘ğ‘

1 Ã—ğ‘‘ğ‘

We start with a claim that if for all inputs x, ğ‘“ and ğ‘” give
the same output at position i, then the dependency mappings
share the same set of indices.

Claim 6. For two well-formed SIMD functions ğ‘“ and ğ‘”,

âˆ€x, ğ‘“ (x) [i] = ğ‘”(x) [i] =â‡’ âˆƒ!ğ‘ƒ, ğ‘ƒ (ğœğ‘“ (i)) = ğœğ‘” (i).

There exists exactly one permutation ğ‘ƒ between dependency
mappings ğœğ‘“ and ğœğ‘”.

Proof. Since ğ‘“ and ğ‘” are SIMD functions, by Definition 12
and the premise, âˆ€x, ğœƒ ğ‘“ (x[ğœğ‘“ (i)]) = ğœƒğ‘” (x[ğœğ‘“ (i)]).

First, we prove the existence of ğ‘ƒ by contraditionâ€”assume
there is no such a ğ‘ƒ: ğ‘ ğ‘’ğ‘¡ (ğœğ‘“ (i)) â‰  ğ‘ ğ‘’ğ‘¡ (ğœğ‘” (i)). Then, there
exists some element j âˆˆ ğœğ‘“ (i) but j âˆ‰ ğœğ‘” (i). We can con-
struct an input Ë†x such that all elements other than j-th are 0;
and Ë†x[j] can be an arbitrary number. Note that by premise,
ğœƒ ğ‘“ ( Ë†x[ğœğ‘“ (i)]) = ğœƒğ‘” ( Ë†x[ğœğ‘” (i)]). By Definition 16, ğ‘“ and ğ‘” are
well-formed, so each input contributes meaningfully. There-
fore, ğœƒ ğ‘“ ([0, . . . , Ë†x[j], . . . , 0]) â‰  ğœƒğ‘” ((cid:174)0), a contradiction to the
premise. This means ğ‘ ğ‘’ğ‘¡ (ğœğ‘“ (i)) = ğ‘ ğ‘’ğ‘¡ (ğœğ‘” (i)).

Finally, we prove ğ‘ƒ is the only possible permutation. By
Fact 5, all elements in ğ‘ ğ‘’ğ‘¡ (ğœğ‘“ (i)), and correspondingly in
ğ‘ ğ‘’ğ‘¡ (ğœğ‘” (i)), are distinct scalars. Therefore, there exists a unique
â–¡
permutation ğ‘ƒ such that ğ‘ƒ (ğœğ‘“ (i)) = ğœğ‘” (i).

Proof. Consider ğ‘– = 0; that is e0 = (cid:174)0.

âˆ€x,ğ‘“ (x) [(cid:174)0] = ğ‘”(x) [(cid:174)0]

â‡’ ğœƒ ğ‘“ (x[ğœğ‘“ ((cid:174)0)]) = ğœƒğ‘” (x[ğœğ‘” ((cid:174)0)])

[Definition 12]

â‡’ ğœƒ ğ‘“ (x[Mğ‘“ Â· (cid:174)0 + bğ‘“ ]) = ğœƒğ‘” (x[Mğ‘” Â· (cid:174)0 + bğ‘”])

[Definition 11, affine transformation]

â‡’ ğœƒ ğ‘“ (x[[bğ‘“ 1, ..., bğ‘“ ğ‘˜ ]) = ğœƒğ‘” (x[[bğ‘”1, ..., bğ‘”ğ‘˜ ]])

[expanding b]

By Claim 6, there exists a unique permutation, say ğ‘ƒ0, such
that ğ‘ƒ0([bğ‘“ 1, . . . , bğ‘“ ğ‘˜ ]) = [bğ‘”1, . . . , bğ‘”ğ‘˜ ].

Similarly, consider ğ‘– = 1 for the premise, which gives

âˆ€x, ğ‘“ (x) [e1] = ğ‘”(x) [e1], where e1 = [1, 0, 0, . . . ] âˆˆ Nğ‘›.

Lemma 7. For SIMD functions ğ‘“ and ğ‘” with well-formed
kernel functions:

âˆ€x, ğ‘“ (x) [e0] = ğ‘”(x) [e0] =â‡’ ğœƒ ğ‘“ (cid:27)ğ‘„ ğœƒğ‘”.

âˆ€x,ğ‘“ (x) [e1] = ğ‘”(x) [e1]

â‡’ ğœƒ ğ‘“ (x[ğœğ‘“ (e1)]) = ğœƒğ‘” (x[ğœğ‘” (e1)])
â‡’ ğœƒ ğ‘“ (x[ğ‘€ğ‘“ Â· e1 + bğ‘“ 1], . . . ) = ğœƒğ‘” (x[ğ‘€ğ‘” Â· e1 + bğ‘”1], . . . )

Proof. Recall e0 = (cid:174)0 âˆˆ Nğ‘›. Then, we have âˆ€x, ğ‘“ (x) [(cid:174)0] =
ğ‘”(x) [(cid:174)0]. Because ğ‘“ andğ‘” are SIMD functions, âˆ€x, ğœƒ ğ‘“ (x[ğœğ‘“ ((cid:174)0)]) =
ğœƒğ‘” (x[ğœğ‘” ((cid:174)0)]). By Claim 6, there exists a permuation, say ğ‘ƒ0,
such that ğ‘ƒ0(ğœğ‘“ ((cid:174)0)) = ğœğ‘” ((cid:174)0).

We denote X = x[ğœğ‘“ ((cid:174)0)]. By Fact 5, ğœğ‘“ ((cid:174)0) doesn't have du-
plicated indices, meaning X traces back to ğ‘˜ unique positions
of x. Hence, X covers all possible inputs of Rğ‘˜ , becaue x is
an arbitrary Rğ‘‘1 Ã—...ğ‘‘ğ‘š tensor.

So, we have:

âˆ€x, ğœƒ ğ‘“ (x[ğœğ‘“ ((cid:174)0)]) = ğœƒğ‘” (x[ğœğ‘” ((cid:174)0)])

â‡’ ğœƒ ğ‘“ (x[ğœğ‘“ ((cid:174)0)]) = ğœƒğ‘” (x[ğ‘ƒ0 (ğœğ‘“ ((cid:174)0))])
â‡’ ğœƒ ğ‘“ (x[ğœğ‘“ ((cid:174)0)]) = ğœƒğ‘” (ğ‘ƒ0(x[ğœğ‘“ ((cid:174)0)]))
â‡’ ğœƒ ğ‘“ (X) = ğœƒğ‘” (ğ‘ƒ0(X))
â‡’ ğœƒ ğ‘“ (cid:27)ğ‘„ ğœƒğ‘”

[Claim 6]

[tensor indexing]

[X = x[ğœğ‘“ ((cid:174)0)]]
[Definition 15]

In addition, ğ‘ƒ0 satisfies permutation requirments in Defini-

tion 15, hence:

By Claim 6, there exists a unique permutation, say ğ‘ƒ1, such
that ğ‘ƒ1([ğ‘€ğ‘“ Â· e1 + bğ‘“ 1, . . . ]) = [ğ‘€ğ‘” Â· e1 + bğ‘”1, . . . ].

We repeat this for all eğ‘–, ğ‘– âˆˆ [0, ğ‘›]. Then, we have:

ğ‘ƒ0([bğ‘“ 1, . . . , bğ‘“ ğ‘˜ ]) = [bğ‘”1, . . . , bğ‘”ğ‘˜ ]
ğ‘ƒ1([Mğ‘“ Â· e1 + bğ‘“ 1, . . . ]) = [Mğ‘” Â· e1 + bğ‘”1, . . . ]
ğ‘ƒ2([Mğ‘“ Â· e2 + bğ‘“ 1, . . . ]) = [Mğ‘” Â· e2 + bğ‘”1, . . . ]
...
ğ‘ƒğ‘› ([Mğ‘“ Â· eğ‘› + bğ‘“ 1, . . . ]) = [Mğ‘” Â· eğ‘› + bğ‘”1, . . . ]






By Claim 9 (which we prove below), all permutations are

equivalent, ğ‘ƒ0 = Â· Â· Â· = ğ‘ƒğ‘›.
Now, we prove ğœğ‘“ (cid:27)ğ‘ƒ0

ğœğ‘”. By Definition 14, we need to
prove âˆ€i âˆˆ Nğ‘›, ğ‘ƒ0(ğœğ‘“ (i)) = ğœğ‘” (i). Notice that eğ‘– 's are standard
basis vectors, so any i is a linear combination of eğ‘– s:

i = ğ‘0e0 + ğ‘1e1 + ğ‘2e2 + Â· Â· Â· + ğ‘ğ‘›eğ‘›

ğ‘ƒ0 âˆˆ ğ‘„,

where ğ‘„ is the permutation set in ğœƒ ğ‘“ (cid:27)ğ‘„ ğœƒğ‘”.

XXX: assume ğ‘› > ğ‘˜?

YC: we can
trivially as-
sume ğ‘› > ğ‘˜,
else the the
kernel func
cannot apply

Lemma 8. For SIMD functions ğ‘“ and ğ‘” with well-formed
kernel functions:

âˆ€x, âˆ€ğ‘– âˆˆ {0, . . . , ğ‘›}, ğ‘“ (x) [eğ‘– ] = ğ‘”(x) [eğ‘– ] =â‡’ ğœğ‘“ (cid:27)ğ‘ƒ ğœğ‘”.

(3)

â–¡

where ğ‘ğ‘– âˆˆ R.

ğœğ‘” (i) = [

ğ‘›
âˆ‘ï¸

ğ‘–=1

ğ‘ğ‘–ğ‘€ğ‘”1 Â· eğ‘– + bğ‘”1,

ğ‘›
âˆ‘ï¸

ğ‘–=1

ğ‘ğ‘–ğ‘€ğ‘”2 Â· eğ‘– + bğ‘”2, . . . ]

= ğ‘ƒ0([

ğ‘›
âˆ‘ï¸

ğ‘–=1

ğ‘ğ‘–ğ‘€ğ‘“ 1 Â· eğ‘– + bğ‘“ 1,

ğ‘›
âˆ‘ï¸

ğ‘–=1

= ğ‘ƒ0(ğœğ‘“ (i))

ğ‘ğ‘–ğ‘€ğ‘“ 2 Â· eğ‘– + bğ‘“ 2, . . . ])

Therefore, ğœğ‘“ (cid:27)ğ‘ƒ0

ğœğ‘”.

â–¡

SOSP '25, October 13â€“16, 2025, Seoul, Republic of Korea

Yunchi Lu, Youshan Miao, Cheng Tan, Peng Huang, Yi Zhu, Xian Zhang, and Fan Yang

Claim 9. Consider the following ğ‘› + 1 equations:

ğ‘ƒ0([bğ‘“ 1, . . . , bğ‘“ ğ‘˜ ]) = [bğ‘”1, . . . , bğ‘”ğ‘˜ ]
ğ‘ƒ1([Mğ‘“ Â· e1 + bğ‘“ 1, . . . ]) = [Mğ‘” Â· e1 + bğ‘”1, . . . ]
ğ‘ƒ2([Mğ‘“ Â· e2 + bğ‘“ 1, . . . ]) = [Mğ‘” Â· e2 + bğ‘”1, . . . ]
...
ğ‘ƒğ‘› ([Mğ‘“ Â· eğ‘› + bğ‘“ 1, . . . ]) = [Mğ‘” Â· eğ‘› + bğ‘”1, . . . ]

(4)






We claim that

âˆ€ğ‘˜, Equation 4 â‡’ ğ‘ƒ0 = Â· Â· Â· = ğ‘ƒğ‘›.

Proof. We prove this claim by contradiction. Without loss
of generality, consider ğ‘ƒ0 is identity mapping, and assume
ğ‘ƒ1 â‰  ğ‘ƒ0, meaning

(cid:40)

[bğ‘“ 1, . . . , bğ‘“ ğ‘˜ ] = [bğ‘”1, . . . , bğ‘”ğ‘˜ ]
ğ‘ƒ1([Mğ‘“ Â· e1 + bğ‘“ 1, . . . ]) = [Mğ‘” Â· e1 + bğ‘”1, . . . ]
Next, we denote ğ‘ƒ1([Mğ‘“ Â·e1+bğ‘“ 1, . . . ]) as [Mğ‘“ Â·e1+bğ‘ƒ 1(ğ‘“ 1), . . . ],
and replace bğ‘”ğ‘– with the corresponding bğ‘“ ğ‘– , so we have:

[Mğ‘“ Â· e1 + bğ‘ƒ 1(ğ‘“ 1), . . . ] = [Mğ‘” Â· e1 + bğ‘“ 1, . . . ]

By rearraning this, we get:




bğ‘“ 1 âˆ’ bğ‘ƒ 1(ğ‘“ 1) = (Mğ‘“ âˆ’ Mğ‘”) Â· e1
bğ‘“ 2 âˆ’ bğ‘ƒ 1(ğ‘“ 2) = (Mğ‘“ âˆ’ Mğ‘”) Â· e1
...
bğ‘“ ğ‘˜ âˆ’ bğ‘ƒ 1(ğ‘“ ğ‘˜ ) = (Mğ‘“ âˆ’ Mğ‘”) Â· e1
Because ğ‘ƒ1 is not identity, by Fact 5, âˆƒğ‘— âˆˆ [1, ğ‘˜], bğ‘“ ğ‘— âˆ’
bğ‘ƒ 1(ğ‘“ ğ‘— ) â‰  (cid:174)0 âˆˆ Rğ‘š, therefore



[bğ‘“ 1 [0], . . . ] âˆ’ [bğ‘ƒ 1( ğ‘“ 1) [0], . . . ]
= [bğ‘“ 2 [0], . . . ] âˆ’ [bğ‘ƒ 1(ğ‘“ 2) [0], . . . ]
...
= [bğ‘“ ğ‘˜ [0], . . . ] âˆ’ [bğ‘ƒ 1( ğ‘“ ğ‘˜ ) [0], . . . ]
â‰  [0, 0, . . . ]






This means at least one dimension, say ğ‘– âˆˆ [0, ğ‘š), have the

following equation:

bğ‘“ 1 [ğ‘–] âˆ’ bğ‘ƒ 1(ğ‘“ 1) [ğ‘–]
= bğ‘“ 2 [ğ‘–] âˆ’ bğ‘ƒ 1(ğ‘“ 2) [ğ‘–]
...
= bğ‘“ ğ‘˜ [ğ‘–] âˆ’ bğ‘ƒ 1(ğ‘“ ğ‘˜ ) [ğ‘–] â‰  0






Consider the value bğ‘“ 1 [ğ‘–] âˆ’ bğ‘ƒ 1(ğ‘“ 1) [ğ‘–], which must be either
positive or negative (â‰  0). Without loss of generality, assume
it is positive; that is bğ‘“ 1 [ğ‘–] > bğ‘ƒ 1(ğ‘“ 1) [ğ‘–]. Since ğ‘ƒ1 is a permua-
tion, one can always locate a corresponding term where bğ‘ƒ 1(ğ‘“1 )
appears as the minuend (the left operand of the subtraction).
This yields another inequality bğ‘ƒ 1(ğ‘“ 1) [ğ‘–] > bğ‘ƒ 1(ğ‘“ ğ‘œ ) [ğ‘–]. By
repeating this reasoning iteratively, we eventually encounter

a subtraction in which bğ‘“ 1 [ğ‘–] appears as the subtrahend (the
right opearand). This results in a a contradition of the form
bğ‘“1 [ğ‘–] > Â· Â· Â· > bğ‘“1 [ğ‘–]. Hence, the contradiction implies that ğ‘ƒ1
must be equavalent to ğ‘ƒ0.

By applying the above reasoning for all eğ‘– s, we conclude

ğ‘ƒ0 = Â· Â· Â· = ğ‘ƒğ‘›.

â–¡

Next, we prove one of our main theorem below.

Theorem 10. For SIMD functions ğ‘“ and ğ‘” with well-formed
kernel functions:

âˆ€x, âˆ€ğ‘– âˆˆ {0, . . . , ğ‘›}, ğ‘“ (x) [ğ‘’ğ‘– ] = ğ‘”(x) [ğ‘’ğ‘– ] =â‡’

âˆ€j, x, ğ‘“ (x) [j] = ğ‘”(x) [j]

Proof. Given the premise:
â€¢ By Lemma 7, ğœƒ ğ‘“ (cid:27)ğ‘„ ğœƒğ‘”.
â€¢ By Lemma 8, ğœğ‘“ (cid:27)ğ‘ƒ ğœğ‘”.
â€¢ By Equation 3, ğ‘ƒ âˆˆ ğ‘„.

Finally, we prove

ğœƒ ğ‘“ (cid:27)ğ‘„ ğœƒğ‘” âˆ§ ğœğ‘“ (cid:27)ğ‘ƒ ğœğ‘” âˆ§ ğ‘ƒ âˆˆ ğ‘„ =â‡’ ğ‘“ = ğ‘”

âˆ€x, âˆ€ğ‘–, ğ‘“ (x) [ğ‘–] = ğœƒ ğ‘“ (x(ğœğ‘“ (ğ‘–)))

= ğœƒğ‘” (ğ‘ƒ (x[ğœğ‘“ (ğ‘–)]))
= ğœƒğ‘” (x[ğ‘ƒ (ğœğ‘“ (ğ‘–))])
= ğœƒğ‘” (x[ğœğ‘” (ğ‘–)])
= ğ‘”(x) [ğ‘–]

[by Definition 12]
[by ğœƒ ğ‘“ (cid:27)ğ‘„ ğœƒğ‘” âˆ§ ğ‘ƒ âˆˆ ğ‘„]
[by tensor indexing rules]
[by ğœğ‘“ (cid:27)ğ‘ƒ ğœğ‘”]

Because for any input x, ğ‘“ (x) and ğ‘”(x) produce the same
result, therefore ğ‘“ = ğ‘”.

â–¡

In the following, we prove the shape reduction equavalence

for reductional operations.

Theorem 11. Given reductional functions ğ‘“âŠ™ and ğ‘”âŠ•,

âˆ€x âˆˆ R2, ğ‘“âŠ™ (x) = ğ‘”âŠ• (x) =â‡’ âˆ€x âˆˆ Rğ‘›, ğ‘› >= 2, ğ‘“âŠ™ (x) = ğ‘”âŠ• (x).

Proof. We prove the lemma by mathematical induction.

Base case. Consider the base case, ğ‘“ (x) = ğ‘”(x)| x âˆˆ R2;
namely, âˆ€x, x[0] âŠ™ x[1] = x[0] âŠ• x[1]. This equality holds
directly from the given premise.

Inductive step. Assume that ğ‘“âŠ™ (x) = ğ‘”âŠ• (x) holds for âˆ€x âˆˆ
Rğ‘˜, ğ‘˜ >= 2. Next, we prove that ğ‘“âŠ™ (x) = ğ‘”âŠ• (x) also holds for
âˆ€x âˆˆ ğ‘…ğ‘˜+1.

TrainVerify: Equivalence-Based Verification for Distributed LLM Training

SOSP '25, October 13â€“16, 2025, Seoul, Republic of Korea

We denote x âˆˆ Rğ‘˜+1 as [x[0..ğ‘˜ âˆ’ 1], x[ğ‘˜]], then:
ğ‘“âŠ™ (x) = ğ‘“âŠ™ (x[0..ğ‘˜ âˆ’ 1]) âŠ™ x[ğ‘˜]
= ğ‘”âŠ• (x[0..ğ‘˜ âˆ’ 1]) âŠ™ x[ğ‘˜]
= ğ‘”âŠ• (x[0..ğ‘˜ âˆ’ 1]) âŠ• x[ğ‘˜]
= ğ‘”âŠ• (x)

[Definition 13]

[Inductive hypothesis]

[Base case]

[Definition 13]

â–¡

C.4 Connecting theorems to practice
In this section, we prove that TrainVerify's checking algo-
rithm is correct, meaning if our verifier (i.e., Z3) accepts, then
the logical DFG is semantically equivalent to the parallelized
DFG executed by machines.

The key idea is that LLM operators are either SIMD func-
tions (Definition 12) or reductional functions (Definition 13),
or (semantically) combination of the two. TrainVerify veri-
fies equivalence by checking two small sub-tensors from the
outputs of the logical DFG and the parallelized DFG. Next, we
prove these sub-tensors are sufficient to guarantee that all other
corresponding parts of the outputs are identical. For example,
by theorem 10 and theorem 11, verifying the equivalence
of the operator MatMul(ğ´, ğµ), where ğ´ âˆˆ ğ‘… [ğ‘š,ğ‘˜ ], ğµ âˆˆ ğ‘… [ğ‘˜,ğ‘›]
with ğ‘š, ğ‘˜, ğ‘› âˆˆ Z+, can be simplified by verifying the case of
ğ‘š, ğ‘›, ğ‘˜ = 2.
