\section{Shape reduction correctness proof}
\label{appx:sec:dim_red}
For complete correctness proof of shape reduction, please refer to our external document at \url{https://arxiv.org/abs/2506.15961}.


\cheng{minor:
- use bold for tensors; normal for scalar
- 0-indexing vs. 1-based indexing (using $\vzero$ seems indicating 0-based)
- indexing is in $\mathbb{I}$ not $\mathbb{R}$. \yomia{consider Natural Numbers $\mathbb{N}$ or Integers $\mathbb{Z}$ rather than $\mathbb{I}$}
}

When a DNN model involves large size tensors, such as popular LLMs, it becomes
infeasible for current solvers (e.g., Z3) to verify its parallel execution
considering the complexity.
In response, we propose a verification for the same model architecture but with reduced tensor shapes, and prove that:
the verification conclusion on the shape-reduced model also applies to the original model with larger tensor shapes.

%\subsection{Formulation}
\subsection{Formalization}

A DNN model consists of multiple operators, such as \texttt{MatMul} and
\texttt{ReLU}, which essentially are functions with data tensors as input and output.
Given input tensor(s), including tensors representing weights,
activation and optimizer state, such a DNN function can produce corresponding
output tensor(s).
Given a DNN function $f$ that executes on a single device, there is an
alternative function $g$ ($g$ is
%\yunchi{potentially} [cheng: we don't want to complicate the situation considering g=f, though the proof trivially works]
different from $f$) that can execute on either
single device sequentially or multiple devices concurrently.
Our goal is to verify that
regardless of the inputs,
$f$ and $g$ can always produce the same results---an equivalence.

\begin{definition}[Tensor]
A \textit{tensor} is an object that generalizes scalars, vectors,
    and matrices to higher dimensions.
    Formally, an order-$n$ tensor (also called an $n$-th order tensor)
    is an element of the tensor space:

\begin{equation}
\mathcal{T} \in \mathbb{R}^{d_1 \times d_2 \times \dots \times d_n}
\end{equation}

where $\mathbb{R}$ represents real number field and $d_1, d_2, \dots, d_n$ denote the
    dimensions along each mode of the tensor.


Tensors are used as primitive data in machine learning.

\end{definition}


\begin{definition}[Functions]
A general function that operates on multiple tensors can be defined as:
\begin{equation}
f: (\mathbb{R}^{d_1^a \times d_2^a \times \dots \times d_n^a}, \mathbb{R}^{d_1^b \times d_2^b \times \dots \times d_m^b}, \cdots) \to \mathbb{R}^{d_1^y \times d_2^y \times \dots \times d_k^y}
\end{equation}
%\yunchi{preferrably use $d_{1,1}$ instead of $d_1'$}
%[cheng: updated; mixing two things in subscript will be confusing]
where $f$ takes one or more tensors as input and outputs a tensor of a potentially different shape.
\end{definition}


For simplicity, we assume a single-tensor input/output for function \f throughout this
proof. The proof can be naturally extended to accommodate multiple input
and output tensors.

We use bold symbols (e.g., $\x$, $\y$) to denote tensors and non-bold symbols (e.g., $x$, $y$) to denote scalars.
We also use zero-based indexing; that is, for a vector $\tb{v}$, the first element is ``$\tb{v}[0]$''.

People have long observed that deep learning operators like matrix multiplication
and convolution are \simd{} (Single-Instruction Multiple-Data):
the operation consists of repeated, homogeneous computations (the ``kernel'') over array elements.
This \simd{} characteristic is the core enabler for our shape reduction mechanism.
Below, we formally define what is a \simd{} function.

Consider a function $\funcfull$, where $\x \in R^{d_1^a\times d_2^a \times \dots \times d_m^a}$
and $\y \in R^{d_1^b \times d_2^b \times \dots \times d_n^b}$.
So, $rank(\x)=m$ and $rank(\y)=n$.


If \f is a \simd{} function,
% \yunchi{agree with Xian, def of $theta$ should not circularly dependent on SIMD property}
a kernel function $\theta$ associated with \f takes a subtensor from $\x$
and outputs a scalar value. Formally:

\begin{definition}[\CF{\kfunc}]
A kerenel function $\theta$ is a function that takes $k$ scalar inputs and produces a single scalar output:
\[
\theta: \mathbb{R}^{k} \to \mathbb{R}.
\]
\label{def:kernelfunc}
\end{definition}

Next, we define which input subtensor is associated with each output element.
%
Consider the same function $\funcfull$.
A \depmap $\tr{}$ associated with \f is a function that maps each index $\ii$ in the output $\y$
to a list of indices in the input $\x$. Formally:

\begin{definition}[\Depmap]
A \depmap $\tr{}$ is an affine transformation that maps
    a vector of integers (an index of tensor $\y$)
    to a list of indices in another tensor (i.e., $\x$):
\begin{align*}
    \tr{}: idx(\y) \in \mathbb{N}^n \to [idx(\x), \dots] \in \mathbb{N}^{k \times m},
\end{align*}
where $idx(\cdot)$ is the indexing function of the tensor; $n$ and $m$ are ranks of $\x$ and $\y$;
and $k$ is the number of inputs in $\theta$.
\label{def:mapping}
\label{def:depmap}
\end{definition}
%\yunchi{should we introduce AFFINE of $\tau$}[cheng:updated]


With \depmap and \kfunc, we define \simd{} functions.


\begin{definition}[\simd{} function]
\label{def:simd_func}

A function
$\funcfull$
is a \simd{} function if,
for each $\y[\ii], \ii \in \mathbb{N}^n$,
\[
\y[\ii] = \theta(\x_{1}, \x_{2}, \dots, \x_{k}),
\]
where $\theta$ is the \kfunc of \f, and
\[
\x_{j} = \x[ \tau(\ii)[j] ],\quad 1\le j \le k
\]
where $\tau$ is the \depmap of \f.
\label{def:simdfunc}
\end{definition}


By fixing a \kfunc $\theta$ and a \depmap $\tau$,
one can define an \simd{} function.
We denote an \simd{} function \f using its \kfunc and \depmap as:
$\y[\ii] = \theta_f(\x[\tau_f(\ii)])$.





%\subsection{Observations: \simd{} characteristics in LLM operations}
\subsection{Observations: LLM operators are \simd{} functions}

Deep Neural Network (DNN) computations are characterized by their application
to high-dimensional data tensors. A closer examination of commonly used DNN
operations reveals that a large number of elements in the output tensor share
the same computational logic, differing only in the specific input elements
they process.
This computational pattern aligns closely with our definition of \simd{} functions.
%Single-Instruction-Multiple-Data (SIMD), a concept from computer architecture.


%\subsubsection{Observation 1: LLM operators are \simd{} functions.}
\subsubsection{Observation 1: LLM operators have \kfunc{}s.}
We observe that each computation operator in the transformer architecture is associated with its own \kfunc,
% We observe that all layers in the transformer architecture are \simd{},
including Feed Forward layers,
Multi-Head Attention layers (without masking),
Add \& Norm layers, ReLU, Softmax,
and Residual addition.
%\yunchi{we might have to change the tone, as not all operators are SIMD, e.g. positional encoding}[cheng: updated]

Consider matrix multiplication (i.e., MatMul) as an example.
Given two matrices $\tb{A} \in \R^{m \times p}$ and $\tb{B} \in \R^{p \times n}$,
the resulting matrix $\tb{C} \in \R^{m \times n}$ has elements $c_{i,j}$ (short for $\tb{C}[i][j]$)
computed by:
$c_{i,j} = \sum_{k=1}^{p} a_{i,k} \cdot b_{k,j}$.
Therefore, MatMul has a \kfunc:
\begin{center}
$\theta(a_{i,1},\dots, a_{i,p}, b_{1,j}, \dots, b_{p,j}) = \sum_{k=1}^{p} a_{i,k} \cdot b_{k,j}$
\end{center}

\subsubsection{Observation 2: LLM operators have \depmap{}s that can be expressed as
affine transformations.}
This property is intuitive, as the ``striding'' of kernel functions across
tensors typically occurs at regular, constant intervals.
Consequently, when the input to the \depmap---corresponding to the output tensor's index---changes,
the resulting input indices change linearly.
That is, the mapping takes the affline transformations:
\[
    \tau(\ii) = [\M_1\cdot \ii + \bb_1, \ \ \dots,\ \  \M_k\cdot \ii + \bb_k].
\]

For example, in the above MatMul case,
the \depmap $\tau^A$ for the first input matrix $\tb{A}$
can be written as affine transformations:
\begin{align*}
    &\tau^{A}(\begin{bmatrix} i \\ j \end{bmatrix}) = [\M_{A1} \begin{bmatrix} i \\ j \end{bmatrix} + \bb_{A1}, \dots, \M_{Ap}\begin{bmatrix} i \\ j \end{bmatrix} + \bb_{Ap} ], \text{where} \\
  &\M_{A1} = \begin{pmatrix}
    1 & 0 \\
    0 & 0
  \end{pmatrix},
  \bb_{A1} = \begin{pmatrix}
    0 \\
    1
  \end{pmatrix}, \dots,
  \M_{Ap} = \begin{pmatrix}
    1 & 0 \\
    0 & 0
  \end{pmatrix},
  \bb_{Ap} = \begin{pmatrix}
    0 \\
    p
  \end{pmatrix}
\end{align*}

Above all,
MatMul is a \simd{} function because it has
\begin{myitemize2}

\item a \kfunc:
    \begin{center}
    $\theta(a_{i,1},\dots, a_{i,p}, b_{1,j}, \dots, b_{p,j}) = \sum_{k=1}^{p} a_{i,k} \cdot b_{k,j};$
    \end{center}

\item a \depmap for each input tensor:
  \begin{align*}
      \tau^{A}([i,j]) &= [[i, k] | 1 \leq k \leq p],\\
      \tau^{B}([i,j]) &= [[k, j] | 1 \leq k \leq p],
  \end{align*}
  where $\tau^A$ and $\tau^B$ are \depmap{}s for input matrix A and B;

\item and MatMul can be expressed as:
\[
    c_{i,j} = \theta(\tb{A}[\tau^A([i,j])] \oplus \tb{B}[\tau^B([i,j])]),
\]
where $\oplus$ represents vector concatenation.

\end{myitemize2}


\begin{fact}
We observe that in practice,
the \depmap $\tau(\cdot)$ does not produce duplicated input indices.
Meaning,
\begin{align*}
    \forall \ii, \tau(\ii) = [\jj_1, \jj_2, \dots, \jj_k] \in \mathbb{N}^{k \times m},\\
    \text{for } 1 \le a \neq b \le k \text{ in }\tau(\ii), \jj_a \neq \jj_b.
\end{align*}
    \label{fact:uniqueindx}
\end{fact}

\begin{fact}
We observe that in practice,
the matrices $\M_j$ in \depmap $\tau(\cdot)$ does not contain all-zero columns.
    Meaning, $\forall \M_j \in \tau(\cdot), \forall j, \M[:,j] \neq 0$
    \label{fact:nonzerocol}
\end{fact}



\subsection{Correctness proof for shape reduction}

This section establishes the correctness of \sys's shape reduction
by proving the equivalence between two data flow graphs (DFGs)
at a reduced scale %subset of dimensions
%\yunchi{`subset of dimensions` feels like ignoring some dimensions} [cheng: updated]
implies equivalence
at the original scale. %across all dimensions.
We denote the original and transformed DFGs---before and after applying parallelization techniques---as functions
\f and \g, respectively.

\subsubsection{Prerequisite relations.}
Before presenting the main theorem, we begin with two equivalent definitions
that serve as the foundation for the proof.


\begin{definition}[\CF{\depeq}]
For two \depmap{}s $\tau_1$ and $\tau_2$,
we call them \depeq, denoted $\tau_1 \cong_P \tau_2$,
if there exists a permutation function $P$,
such that
\[
\forall i, \ \ \tau_1(i) = P(\tau_2(i))
\]
    \label{def:depeq}
\end{definition}

\CF{\depeq} captures LLM operators with commutative
properties, where permuting the inputs does not affect the output. Similarly,
we need to define a corresponding equivalence relation for \kfunc{}s.

\begin{definition}[\CF{\kerneq}]
For two \kfunc{}s $\theta_1$ and $\theta_2$,
we call them \kerneq, denoted $\theta_1 \cong_Q \theta_2$,
if there exists a non-empty set $Q$ of permutation functions,
such that
\[
\forall P\in Q,\ \forall \x, \ \ \theta_1(\x) = \theta_2(P(\x))
\]
    \label{def:kerneq}
\end{definition}

% \CF{\kerneq} captures LLM operators with commutative
% properties, where permuting the inputs does not affect the output.
% Similarly, we need to define a corresponding equivalence relation for \depmap{}s.






\yunchi{\XXX difficult to interpret this formula}
\begin{definition}[Well-formed \kfunc]
We call a \kfunc $\theta$ well-formed if,
    \begin{align*}
        \exists \x,\x', \forall i, \x[i] \neq \x'[i] &\text{ and } \forall j\neq i, \x[j]=\x'[j]\\
        & \theta(\x) \neq \theta(\x')
    \end{align*}
    \label{def:wellform}
\end{definition}

Essentially, all elements in the inputs to the well-formed \kfunc{}s
contribute to the final output.
There is no such input element that does not influcent the output.




\subsubsection{Premises from SMT solver.}
In \sys, we use an SMT solver (Z3) to verify that a shape-reduced model preserves parallelization equivalence.
Specifically, if the solver returns \texttt{sat}, it proves that for all inputs,
the logical dataflow graph of the shape-reduced model is equivalent to that of the parallelized version.

This result yields a premise for each stage (\S\ref{subsec:stage_verify}) in \sys of the form:
\begin{align*}
    \forall \x,\forall \ii \in \mathcal{I},\ &f(\x)[\ii] = g(\x)[\ii], \\
    \text{ where } &\mathcal{I} = \{\sum_{j=0}^n a_j \e_j \mid a_j \in \{0,1\} \text{ for all } j\} \\
\end{align*}
%\begin{align*}
%    \forall \x,\ f(\x)[\e_i] &= g(\x)[\e_i], \text{ for } 0 \le i \le n,
%\end{align*}
\yunchi{not all $e_i$}\CP{I don't understand the comment}
In the equation,
$\e_i$ denotes the standard basis vectors in $\mathbb{R}^n$, defined as:
\[
(\e_i)_j =
\begin{cases}
1 & \text{if } j = i, \\
0 & \text{otherwise}.
\end{cases}
\]
Each $\e_i \in \mathbb{N}^n$ is a column vector with a single $1$ in the i-th position and $0$ elsewhere, except for $e_0$ which is all $0$s.
For example,
\[
    \e_0 = \begin{pmatrix}
      0  \\
      \vdots \\
      0
    \end{pmatrix}_{n\times 1},
    \e_1 = \begin{pmatrix}
      1  \\
      \vdots \\
      0
    \end{pmatrix}_{n\times 1}, \dots,
    \e_n = \begin{pmatrix}
      0  \\
      \vdots \\
      1
    \end{pmatrix}_{n\times 1}
\]

The above premise holds due to Algorithm~\ref{algo:shape_infer}, line~\ref{ln:semantic},
where \sys enforces that,
for any output dimension of each operator---excluding those not involved in
computation (e.g., batch dimensions or element-wise operations)---both the
logical and parallelized dataflow graphs retain a size of at least two in those
dimensions. %in the shape-reduced models.
Meanwhile, the equivalence for abitrary input $\x$ is established by the symbolic execution.



\subsubsection{Main proofs.}
We now present the main proof of shape reduction correctness.
The argument proceeds in three steps:
\begin{myenumerate2}
    \item We first prove $\theta_f \cong_Q \theta_g$ given the above premise.
    \item We then prove $\tau_f \cong_P \tau_g$ based on the premise.
    \item Finally, we apply $\theta_f \cong_Q \theta_g$ and $\tau_f \cong_P \tau_g$ to establish the shape reduction theorem.
\end{myenumerate2}

%\yunchi{better to distinguish $\y_f$ and $\y_g$}[cheng: updated; using y' to lower readers' memory burden]
\noindent
Next, we consider $\funcfull$ and $g(\x) \to \y'$,
where $\x \in R^{d_1^a\times d_2^a \times \dots \times d_m^a}$
and $\y,\y' \in R^{d_1^b\times d_2^b \times \dots \times d_n^b}$.
So, $rank(\x)=m$ and $rank(\y)=n$.




% helper claim
We start with a claim that if for all inputs $\x$,
$f$ and $g$ give the same output at position $\ii$,
then the \depmap{}s share the same set of indices.

\begin{claim}
\emph{For two well-formed \simd{} functions $f$ and $g$,}
\begin{align*}
    \forall \x, f(\x)[\ii]=g(\x)[\ii] \implies \exists! P, P(\tau_f(\ii)) = \tau_g(\ii).
\end{align*}
    There exists exactly one permutation $P$ between \depmap{}s $\tau_f$ and $\tau_g$.
    \label{claim:set}
\end{claim}

%\yunchi{it should -> more strict relation beyond set equivalence, e.g. P or without set?}
% [cheng: updated]

\begin{proof}
    Since $f$ and $g$ are \simd{} functions,
    by Definition~\ref{def:simdfunc} and the premise,
    $\forall \x, \theta_f(\x[\tau_f(\ii)]) = \theta_g(\x[\tau_f(\ii)])$.


    First, we prove the existence of $P$ by contradition---assume
    there is no such a $P$: $set(\tau_f(\ii)) \neq set(\tau_g(\ii))$.
    Then, there exists some element
    $\jj \in \tau_f(\ii)$ but $\jj \not\in \tau_g(\ii)$.
    We can construct an input $\hat{\x}$ such that
    all elements other than $\jj$-th are $0$; and $\hat{\x}[\jj]$
    can be an arbitrary number.
    Note that by premise, $\theta_f(\hat{\x}[\tau_f(\ii)]) = \theta_g(\hat{\x}[\tau_g(\ii)])$.
    %
    By Definition~\ref{def:wellform}, $f$ and $g$ are well-formed,
    so each input contributes meaningfully.
    Therefore, $\theta_f([0,\dots,\hat{\x}[\jj],\dots,0]) \neq \theta_g(\vzero)$,
    a contradiction to the premise.
    This means $set(\tau_f(\ii)) = set(\tau_g(\ii))$.

    Finally, we prove $P$ is the only possible permutation.
    By Fact~\ref{fact:uniqueindx},
    all elements in $set(\tau_f(\ii))$, and correspondingly in $set(\tau_g(\ii))$, are distinct scalars.
    Therefore, there exists a unique permutation $P$ such that
    $P(\tau_f(\ii)) = \tau_g(\ii)$.
\end{proof}

\yunchi{relu as a contradiction}
\CP{why? for ReLU, $\theta: \mathcal{R}\to\mathcal{R}$; how it is a counterexample (assuming this is what you mean)}


% lemma 1
\yunchi{what if $theta$ contains branching that is value/index-dependent}
\CP{notice the definition of $\theta$, how would it be aware of index?}

\begin{lemma}
For \simd{} functions \f and \g with well-formed \kfunc{}s:
\[
    %\forall \x, \forall i \in \{0,\dots,n\}, f(\x)[\e_i]=g(\x)[\e_i] \implies \theta_f \cong_Q \theta_g.
    \forall \x, f(\x)[\e_0]=g(\x)[\e_0] \implies \theta_f \cong_Q \theta_g.
\]
\label{lemma:kerneleq}
\end{lemma}

\begin{proof}
    Recall $\e_0 = \vzero \in \mathbb{N}^n$.
    Then, we have
    $\forall \x, f(\x)[\vzero] = g(\x)[\vzero]$.
    Because $f$ and $g$ are \simd{} functions,
    $\forall \x, \theta_f(\x[\tau_f(\vzero)]) = \theta_g(\x[\tau_g(\vzero)])$.
    By Claim~\ref{claim:set},
    there exists a permuation, say $P_0$,
    such that $P_0(\tau_f(\vzero)) = \tau_g(\vzero)$.

    We denote $\mathcal{X}=\x[\tau_f(\vzero)]$.
    By Fact~\ref{fact:uniqueindx}, $\tau_f(\vzero)$ doesn't have duplicated indices,
    meaning $\mathcal{X}$ traces back to $k$ unique positions of $\x$.
    Hence, $\mathcal{X}$ covers all possible inputs of $\mathbb{R}^k$,
    becaue $\x$ is an arbitrary $\mathbb{R}^{d_1\times\dots d_m}$ tensor.

    So, we have:
\begin{align*}
    \forall \x, &\ \ \theta_f(\x[\tau_f(\vzero)]) = \theta_g(\x[\tau_g(\vzero)]) \\
                &\Rightarrow \theta_f(\x[\tau_f(\vzero)]) = \theta_g(\x[P_0(\tau_f(\vzero))]) & \text{[Claim~\ref{claim:set}]}\\
                &\Rightarrow \theta_f(\x[\tau_f(\vzero)]) = \theta_g(P_0(\x[\tau_f(\vzero)])) &\text{[tensor indexing]}\\
                &\Rightarrow \theta_f(\mathcal{X}) = \theta_g(P_0(\mathcal{X})) &[\mathcal{X}=\x[\tau_f(\vzero)]] \\
                &\Rightarrow \theta_f \cong_Q \theta_g &[\text{Definition~\ref{def:kerneq}}]
\end{align*}

    In addition, $P_0$ satisfies permutation requirments in Definition~\ref{def:kerneq}, hence:
    \begin{align}
        P_0 \in Q,
        \label{eq:pinq}
    \end{align}
    where $Q$ is the permutation set in $f \cong_Q g$.
\end{proof}




% lemma 2
\begin{lemma}
For \simd{} functions \f and \g with well-formed \kfunc{}s:
    \begin{align*}
        \forall \x,\forall \ii \in \mathcal{I},\ &f(\x)[\ii] = g(\x)[\ii], \\
        \text{ where } &\mathcal{I} = \{\text{$\sum_{j=0}^n$} a_j \e_j \mid a_j \in \{0,1\} \text{ for all } j\} \\
        & \Rightarrow \tau_f \cong_P \tau_g.
    \end{align*}

% \[
%     \forall \x, \forall i \in \{0,\dots,n\}, f(\x)[\e_i]=g(\x)[\e_i] \implies \tau_f \cong_P \tau_g.
% \]

\label{lemma:mappingeq}
\end{lemma}

\begin{proof}
    Consider $i=0$; that is $\e_0 = \vzero$.
    \begin{align*}
        \forall \x, &f(\x)[\vzero] = g(\x)[\vzero] \\
            &\Rightarrow \theta_f(\x[\tau_f(\vzero)]) = \theta_g(\x[\tau_g(\vzero)]) \\
            & \makebox[0.9\linewidth][r]{\text{[Definition~\ref{def:simdfunc}]}}\\
            &\Rightarrow \theta_f(\x[\M_f\cdot\vzero+\bb_f]) = \theta_g(\x[\M_g\cdot\vzero+\bb_g])\\
            & \makebox[0.9\linewidth][r]{\text{[Definition~\ref{def:depmap}, affine transformation]}} \\
            &\Rightarrow \theta_f(\x[[b_{f1},\text{...}, b_{fk}]) = \theta_g(\x[[b_{g1},\text{...},b_{gk}]]) \\
            &  \makebox[0.9\linewidth][r]{\text{[expanding $\bb$]}} \\
            % &\Rightarrow \theta_f(\x[b_{f1}],\text{...}, \x[b_{fk}]) = \theta_g(\x[b_{g1}],\text{...},\x[b_{gk}]) \\
            % &  \makebox[0.9\linewidth][r]{\text{[tensor indexing rules]}} \\
    \end{align*}
    By Claim~\ref{claim:set},
    there exists a unique permutation, say $P_0$, such that
    $P_0([b_{f1},\dots,b_{fk}]) = [b_{g1},\dots,b_{gk}]$.
    %Also, by Lemma~\ref{lemma:kerneleq} and Equation~\ref{eq:pinq},
    %$f \cong_Q g  \land P_0 \in Q$.

    Similarly, consider $i=1$ for the premise,
    which gives $\forall \x, f(\x)[\e_1] = g(\x)[\e_1]$,
    where $\e_1=[1,0,0,\dots] \in \mathbb{N}^n$.
    \begin{align*}
        \forall \x, & f(\x)[\e_1] = g(\x)[\e_1] \\
        &\Rightarrow \theta_f(\x[\tau_f(\e_1)]) = \theta_g(\x[\tau_g(\e_1)]) \\
        &\Rightarrow \theta_f(\x[M_{f1}\cdot \e_1 + b_{f1}], \dots) = \theta_g(\x[M_{g1}\cdot \e_1 + b_{g1}], \dots)
        %&\Rightarrow \theta_f(\x[M_{f1}[:,1] + b_{f1}], \dots) = \theta_g(\x[M_{g1}[:,1] + b_{g1}], \dots)
    \end{align*}
    By Claim~\ref{claim:set},
    there exists a unique permutation, say $P_1$, such that
    $P_1([M_{f1}\cdot \e_1 + b_{f1},\dots]) = [M_{g1}\cdot \e_1 + b_{g1}, \dots]$.
    %By Lemma~\ref{lemma:kerneleq} and Equation~\ref{eq:pinq},
    %$f \cong_Q g  \land P_1 \in Q$.

    We repeat this for all $\mathcal{I} = \{\text{$\sum_{j=0}^n$} a_j \e_j \mid a_j \in \{0,1\} \text{ for all } j\}$,
    Then, we have:
    \begin{align*}
        \begin{cases}
            [b_{f1},\dots,b_{fk}] = P_0([b_{g1},\dots,b_{gk}])\\
            [\M_{f1}\cdot \e_1 + b_{f1},\dots] = P_1([\M_{g1}\cdot \e_1 + b_{g1}, \dots])\\
            [\M_{f1}\cdot \e_2 + b_{f1},\dots] = P_2([\M_{g1}\cdot \e_2 + b_{g1}, \dots])\\
            [\M_{f1}\cdot (\e_1+\e_2) + b_{f1},\dots] = P_3([\M_{g1}\cdot (\e_1+\e_2) + b_{g1}, \dots])\\
            \vdots\\
            [\M_{f1}\cdot (\sum_{i=0}^{n}\e_i) + b_{f1},\dots] = P_{2^n-1}([\M_{g1}\cdot (\sum_{i=0}^n\e_i) + b_{g1}, \dots])
        \end{cases}
    \end{align*}
    % \begin{align}
    %     \begin{cases}
    %         set(b_{f1},\dots,b_{fk}) = set(b_{g1},\dots,b_{gk})\\
    %         set(M_{f1}\cdot \e_1 + b_{f1},\dots) = set(M_{g1}\cdot \e_1 + b_{g1}, \dots)\\
    %         \vdots\\
    %         set(M_{f1}\cdot \e_n + b_{f1},\dots) = set(M_{g1}\cdot \e_n + b_{g1}, \dots)
    %     \end{cases}
    %     \label{eq:manysets}
    % \end{align}
    By Claim~\ref{claim:hard2prove} (which we prove below),
    all permutations are equivalent, $P_0=\dots=P_{2^n-1}$.

    Now, we prove $\tau_f \cong_{P_0} \tau_g$.
    By Definition~\ref{def:depeq},
    we need to prove $\forall \ii \in \mathbb{N}^n, \tau_f(\ii) = P_0(\tau_g(\ii))$.
    Notice that $\e_i$s are standard basis vectors,
    so any $\ii$ is a linear combination of $\e_i$s:
    \[
      \ii = a_0\e_0 + a_1\e_1 + a_2\e_2 + \dots + a_n\e_n
    \]
    where $a_i \in \mathbb{R}$.
    \begin{align*}
        \tau_f(\ii) &=\ [\sum_{i=1}^{n} a_iM_{f1}\cdot \e_i + b_{f1}, \sum_{i=1}^{n} a_iM_{f2}\cdot \e_i + b_{f2}, \dots  ] \\
        &=\ P_0([\sum_{i=1}^{n} a_iM_{g1}\cdot \e_i +b_{g1}, \sum_{i=1}^{n} a_iM_{g2}\cdot \e_i + b_{g2}, \dots]) \\
        &=\ P_0(\tau_g(\ii))
    \end{align*}

    Therefore, $\tau_f \cong_{P_0} \tau_g$.


\end{proof}

\begin{claim}
Consider the following $2^n$ equations:
    \begin{align}
        \begin{cases}
            [\bb_{f1},\dots,\bb_{fk}] = P_0([\bb_{g1},\dots,\bb_{gk}])\\
            [\M_{f1}\cdot \e_1 + \bb_{f1},\dots] = P_1([\M_{g1}\cdot \e_1 + \bb_{g1}, \dots])\\
            [\M_{f1}\cdot \e_2 + \bb_{f1},\dots] = P_2([\M_{g1}\cdot \e_2 + \bb_{g1}, \dots])\\
            [\M_{f1}\cdot (\e_1+\e_2) + \bb_{f1},\dots] = P_3([\M_{g1}\cdot (\e_1+\e_2) + \bb_{g1}, \dots])\\
            \vdots\\
            [\M_{f1}\cdot (\sum_{i=0}^{n}\e_i) + \bb_{f1},\dots] = P_{2^n-1}([\M_{g1}\cdot (\sum_{i=0}^n\e_i) + \bb_{g1}, \dots])
        \end{cases}
        \label{eq:hardeq}
    \end{align}
    where
    \begin{myitemize}
        \item $\{\bb_{f1},\dots,\bb_{fk}\}$ are unique vectors of size $\mathbb{R}^m$;
        %\item $\{\M_{f1},\dots,\M_{fk}\}$ and $\{\M_{f1},\dots,\M_{fk}\}$ are unqiue matrices of size $\mathbb{R}^{m\times n}$;
        \item elements in each list are unique (Fact~\ref{fact:uniqueindx});
        \item $\e_i$s are standard basis vectors;
        \item $P_0, \dots, P_{2^n-1}$ are permuations for $k$ items.
    \end{myitemize}

    We claim that
 \[
     \forall k, \text{Equation~\ref{eq:hardeq}} \Rightarrow P_0=\cdots=P_{2^n-1}.
\]
    \label{claim:hard2prove}
\end{claim}

\begin{proof}
    When $k=1$,
    the only possible permutation is the identity (or none exists),
    so the conclusion is trivially true.

    For $k\ge 2$, we prove the claim by mathematical induction.

    \heading{Base case: $k=2$.}
    When $k=2$, Equation~\ref{eq:hardeq} becomes:
    \begin{align}
        \begin{cases}
            [\bb_{f1}, \bb_{f2}] = P_0([\bb_{g1},\bb_{g2}])\\
            [\M_{f1} \e_1 + \bb_{f1},\M_{f2} \e_1 + \bb_{f2}] = P_1([\M_{g1} \e_1 + \bb_{g1}, \M_{g2} \e_1 + \bb_{g2}])\\
            [\M_{f1} \e_2 + \bb_{f1},\M_{f2} \e_2 + \bb_{f2}] = P_2([\M_{g1} \e_2 + \bb_{g1}, \M_{g2} \e_2 + \bb_{g2}])\\
            [\M_{f1} (\e_1+\e_2) + \bb_{f1},\M_{f2} (\e_1+\e_2) + \bb_{f2}] \\
            \quad\quad\quad\quad= P_3([\M_{g1} (\e_1+\e_2) + \bb_{g1}, \M_{g2} (\e_1+\e_2) + \bb_{g2}])\\
            \vdots\\
        \end{cases}
        \label{eq:keq2}
    \end{align}
    Next, we focus on $\e_1,\e_2$ to prove $P_0=P_1=P_2=P_3$ (i.e., the four equations immediately above).
    The proof naturally extends other $P_i$s by picking all other pairs of $\e_i,\e_j$.

    Notice that for $k=2$, there are two possible permutation: identity and swaping.
    Without loss of generality, assume $P_0$ is identity.
    Then, $\bb_{f1}=\bb_{g1}, \bb_{f2}=\bb_{g2}$.

    Now, consider $P_1$. We prove by contradiction: assume $P_1 \neq P_0$, meaning $P_1$ is swapping.
    Therefore, we have:
    \begin{align*}
        \begin{cases}
            \M_{f1} \e_1 + \bb_{f1} = \M_{g2} \e_1 + \bb_{g2}\\
            \M_{f2} \e_1 + \bb_{f2} = \M_{g1} \e_1 + \bb_{g1}\\
        \end{cases}
    \end{align*}
    Since $\bb_{f1}=\bb_{g1}, \bb_{f2}=\bb_{g2}$,
    \begin{align*}
        \begin{cases}
            \M_{f1} \e_1 + \bb_{f1} = \M_{g2} \e_1 + \bb_{f2}\\
            \M_{f2} \e_1 + \bb_{f2} = \M_{g1} \e_1 + \bb_{f1}\\
        \end{cases}
    \end{align*}
    By rearranging the equations,
    \begin{align*}
        \begin{cases}
            (\M_{f1}-\M_{g2}) \e_1 = \bb_{f2} - \bb_{f1} \\
            (\M_{g1}-\M_{f2}) \e_1 = \bb_{f2} - \bb_{f1} \\
        \end{cases}
    \end{align*}
    Because $\{\bb_{f1},\dots,\bb_{fk}\}$ are unique vectors,
    $\bb_{f2}-\bb_{f1} \neq 0$. Because $\e_1=[1,0,0,\dots]$, we conclude:
    \begin{myitemize2}
        \item (a) $\M_{f1}[:,1] - \M_{g2}[:,1] \neq 0$
        \item (b) $\bb_{f2} - \bb_{f1} = [x,0,0,\dots], x\neq 0$.
    \end{myitemize2}
    Now, consider $P_3$ and there are two possiblities:
    \begin{enumerate}
        \item $P_3$ is identity:
            then, $\M_{f1}(\e_1+\e_2)+\bb_{f1} = \M_{g1}(\e_1+\e_2)+\bb_{g1}$,
            hence $(\M_{f1}-\M_{g1})(\e_1+\e_2) = 0$, a contradiction to
                above (a) $\M_{f1}[:,1] - \M_{g2}[:,1] \neq 0$.
        \item $P_3$ is swapping:
            then, $\M_{f1}(\e_1+\e_2)+\bb_{f1} = \M_{g2}(\e_1+\e_2)+\bb_{g2}$,
            hence $(\M_{f1}-\M_{g2})(\e_1+\e_2) = \bb_{f2} - \bb_{f1}$.
            Because of the above (b) $\bb_{f2} - \bb_{f1} = [x,0,0,\dots]$,
            \begin{align*}
                & (\M_{f1}-\M_{g2})\e_2 = 0 \Rightarrow \M_{f1}[:,2] = \M_{g2}[:,2] = 0
              %& \Rightarrow \M_{f1}\e_2 + \bb_{f1}= \M_{g2}\e_2 + \bb_{f1}
            \end{align*}
            This is a contradiction to Fact~\ref{fact:nonzerocol}
            % \begin{myitemize}
            %     \item If $P_2$ is identity,
            %         $\M_{f1}\e_2 + \bb_{f1} = \M_{g1}\e_2 + \bb_{f1}$,
            %         hence
            %         $\M_{f1}\e_2 = \M_{g1}\e_2 = \M_{g2}\e_2$,
            %         (\XXX)
            %     \item  If $P_2$ is swapping,
            %         $\M_{f1}\e_2 + \bb_{f1} = \M_{g2}\e_2 + \bb_{g2} \neq \M_{g2}\e_2 + \bb_{f1}$
            %         (because of $\bb_{f2} = \bb_{g2} \neq \bb_{f1}$).
            % \end{myitemize}
    \end{enumerate}
    Above all, $P_1 = P_0$.

    Because $\e_2$ and $\e_1$ are interchangable in the above proof,
    swapping $\e_1$ and $\e_2$ everywhere in the proof will conclude $P_2 = P_0$.

    Given $P_0=P_1=P_2$, by adding the second and the third equations and subtracting the first euqation
    in Equation~\ref{eq:keq2},
    we have $P_3 = P_0 = P_1 = P_2$.
    This concludes the proof for the base case, $k=2$.

    \heading{Inductive hypothesis.}
    Assume $P_0 = P_1 = \dots = P_{2^n-1}$ for $k=p$,
    where $p$ is some arbitrary positive integer $p > 2 \in \mathbb{N}$.

    \heading{Inductive step: $k=p+1$.}
    \XXX



\end{proof}



Finally, we prove the main theorem below.

\begin{theorem}
Let \f and \g be functions composed by LLM operators.
\begin{align*}
    \forall \x, \forall i \in \{0,\dots,n\}, f(\x)[e_i]=g(\x)[e_i] &\implies \\
       \forall \jj,\x, \ &f(\x)[\jj]=g(\x)[\jj]
\end{align*}
\label{theorem:main}
\end{theorem}

\begin{proof}
Given the premise:
    \begin{myitemize2}
        \item By Lemma~\ref{lemma:kerneleq}, $\theta_f \cong_Q \theta_g$.
        \item By Lemma~\ref{lemma:mappingeq}, $\tau_f \cong_P \tau_g$.
        \item By Equation~\ref{eq:pinq}, $P \in Q$.
    \end{myitemize2}

% Next,
% by our observation 2 and the precondition, we conclude $\tau_f \cong_P \tau_g$.
% From the precondition, we can derive that $\exists P, \tau_f(i) \cong_P \tau_g(i)$.
% By observation 2, we know that the $P$ applies to all
% dimensions (i.e., $\tau_f \cong_P \tau_g$) due to the linear transformation.
%
% Then, using the precondition and $\tau_f \cong_P \tau_g$, we prove $P \in Q$
% by contradiction: assume $P \not\in Q$, then
% $\exists \x', \theta_f(\x') \neq \theta_g(P(\x'))$.
% Because the precondition is true for all input tensor,
% we can construct an input tensor $\mathcal{X}$ to \f and \g,
% such that $\mathcal{X}[\tau_f(j)] = \x'$.
% Now we have:
%     \begin{align*}
%         f(\mathcal{X})[j] &= \theta_f(\mathcal{X}[\tau_f(j)]) \\
%           &= \theta_f(\x') \neq \theta_g(P(\x')) \\
%           & = \theta_g(P(\mathcal{X}[\tau_g(j)]))   = g(\mathcal{X})[j],
%     \end{align*}
% which is a contradiction to the precondition.

Finally, we prove
\[
\theta_f \cong_Q \theta_g\ \land\
\tau_f \cong_P \tau_g \ \land \
P\in Q \implies f=g
\]

\begin{align*}
\forall \x, \forall i, f(\x)[i]
   &= \theta_f(\x(\tau_f(i)))      &\text{[by Definition~\ref{def:simdfunc}]}\\
   &= \theta_g(P(\x[\tau_f(i)]))   &\text{[by $\theta_f\cong_Q \theta_g \land P\in Q$]}\\
   &= \theta_g(\x[P(\tau_f(i))])   &\text{[by tensor indexing rules]}\\
   &= \theta_g(\x[\tau_g(i)])      &\text{[by $\tau_f \cong_P \tau_g$]}\\
   &= g(\x)[i]
\end{align*}
Because for any input $\x$, $f(\x)$ and $g(\x)$ produce the same result,
therefore $f=g$.

\end{proof}
